# ML-DL-RL-Learning
我的AI学习实战之旅。机器学习、深度学习、强化学习。反正都是要学习的。

一切为了好玩的项目！


## 当前内容

1.  KNN 实现 简单的分类 与 回归
2.  决策树DecisionTree 实现 判断特征的重要性 + 可视化决策树
3.  Q-learning 实现 AI走迷宫 （Success）
4.  尝试 Q-learning 实现 井字棋 （好蠢的AI）
5.  尝试 DQN 实现 五子棋 ing （Failure）
6.  动态规划版本的Q值求解
7.  使用 蒙特卡洛方法和时序差分方法实现21点游戏应用
8.  使用 DQN、AC方法求解 gymnasium 的Atari 游戏
9.  使用 DQN 实现 AI走 随机迷宫 （Success）


### 学习日志

之前的暑假学习的强化学习（QLearning 五子棋）忘记记录了。就从今天开始吧。
#### 2023.10.6 (multi-armed bandit，MAB) 多臂老虎机 问题探索

Project Path: RL-Learning/MAB

有一个拥有 K 根拉杆的老虎机，拉动每一根拉杆都对应一个关于奖励的概率分布R。我们每次拉动其中一根拉杆，就可以从该拉杆对应的奖励概率分布中获得一个奖励r。我们在各根拉杆的奖励概率分布未知的情况下，从头开始尝试，目标是在操作T次拉杆后获得尽可能高的累积奖励。由于奖励的概率分布是未知的，因此我们需要在“探索拉杆的获奖概率”和“根据经验选择获奖最多的拉杆”中进行权衡。“采用怎样的操作策略才能使获得的累积奖励最高”便是多臂老虎机问题。

我们定义了累计懊悔（regret）：拉动当前拉杆的动作和最优拉杆的期望奖励差。

使得MAB问题目标等价于最小化累计懊悔。

实现的solver中共有三种算法:

- ϵ-贪心算法（其中固定的ϵ会令累计懊悔线性增长，而随着时间反比例降低的ϵ则可以实现对数级别的效果。）
- 上置信界算法（霍夫丁不等式：尝试少的拉杆，自然会有更高的期望上界。）
- 汤普森采样算法（采样计算每根拉杆的奖励概率）



#### 2023.10.20 (Dynamic Programming) 动态规划算法

基于动态规划的强化学习算法主要有两种：一是策略迭代（policy iteration），二是价值迭代（value iteration）。

其中，策略迭代由两部分组成：策略评估（policy evaluation）和策略提升（policy improvement）。

具体来说，策略迭代中的策略评估使用贝尔曼期望方程来得到一个策略的状态价值函数，这是一个动态规划的过程；而价值迭代直接使用贝尔曼最优方程来进行动态规划，得到最终的最优状态价值。

Project Path: RL-Learning/DynamicProgramming

条件：（model-based）
基于动态规划的这两种强化学习算法要求事先知道环境的状态转移函数和奖励函数，也就是需要知道整个马尔可夫决策过程。
在这样一个白盒环境中，不需要通过智能体和环境的大量交互来学习，可以直接用动态规划求解状态价值函数。

#### 2023.11.09 （MC TD方法实践）

使用蒙特卡洛和时序差分（Q-learning和Sarsa）的方法去实现一个超简易版本的21点游戏。

其中的庄家策略固定，所以这个游戏没什么博弈的意思在里面。

这次的封装较好，可以并在一起执行（不存在网络）。

Project Path: RL-Learning/easy21


#### 2024.1.10 （GYM Atari游戏——DQN & AC）

应用DQN和AC去解Atari游戏，游戏过难收敛，或者本身实现出现了意外或超参不对。训练很久才能看到一点点上升。

大概率怀疑是探索与开发的平衡问题。（再训练时提升较明显。）

Project Path: RL-Learning/GYM





#### 2024.1.26 

- **蒙特卡洛方法**：一种通过从**完整的样本序列**（一整个回合）中学习以估计状态的价值或策略的强化学习方法。

- **时序差分（TD）方法**：在蒙特卡洛方法的基础上改进，通过从**部分序列**（单步或多步）学习来估计状态的价值，允许更频繁的更新且不需要等待回合结束，但不可以视作单个Actor。（蒙特卡洛方法就是 TD(无穷步) ）

- **Actor-Critic方法**：在时序差分（TD）方法的基础上进一步改进，结合了**值函数（Critic）**和策略函数（Actor），允许同时优化策略和价值估计。

- **DDPG（深度确定性策略梯度）**：在Actor-Critic方法的基础上进一步引入了**确定性策略**，使其适用于连续动作空间。
  - 确定性策略就是说Actor输出的不是不同动作的采取概率了，而是直接输出一个具体的动作。

  - 采取了DQN的**经验回放**和**目标网络**的操作。

- **TD3（双重深度确定性策略梯度）**：在DDPG的基础上，通过引入**双重Critic**网络和策略平滑技术，进一步提高了算法在连续动作空间问题中的稳定性和性能。



#### 2.7 决策树

增加决策树的分类和回归代码，可惜波士顿房价有点道德问题，没看到经典例子。
但是增加了可视化的部分，可了解决策分类过程。