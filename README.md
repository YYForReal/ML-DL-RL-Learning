# ML-DL-RL-Learning
我的AI学习实战之旅。机器学习、深度学习、强化学习。反正都是要学习的。

一切为了好玩的项目！

## 依赖

gym 0.23.0






## 实战项目

1.  KNN 实现 简单的分类 与 回归
2.  决策树DecisionTree 实现 判断特征的重要性
3.  Q-learning 实现 AI走迷宫
4.  尝试 Q-learning 实现 井字棋 （好蠢的AI）
5.  尝试 DQN 实现 五子棋 ing


### 学习日志

之前的暑假学习的强化学习（QLearning 五子棋）忘记记录了。就从今天开始吧。
#### 2023.10.6 (multi-armed bandit，MAB) 多臂老虎机 问题探索

Project Path: RL-Learning/MAB

有一个拥有 K 根拉杆的老虎机，拉动每一根拉杆都对应一个关于奖励的概率分布R。我们每次拉动其中一根拉杆，就可以从该拉杆对应的奖励概率分布中获得一个奖励r。我们在各根拉杆的奖励概率分布未知的情况下，从头开始尝试，目标是在操作T次拉杆后获得尽可能高的累积奖励。由于奖励的概率分布是未知的，因此我们需要在“探索拉杆的获奖概率”和“根据经验选择获奖最多的拉杆”中进行权衡。“采用怎样的操作策略才能使获得的累积奖励最高”便是多臂老虎机问题。

我们定义了累计懊悔（regret）：拉动当前拉杆的动作和最优拉杆的期望奖励差。

使得MAB问题目标等价于最小化累计懊悔。

实现的solver中共有三种算法:

- ϵ-贪心算法（其中固定的ϵ会令累计懊悔线性增长，而随着时间反比例降低的ϵ则可以实现对数级别的效果。）
- 上置信界算法（霍夫丁不等式：尝试少的拉杆，自然会有更高的期望上界。）
- 汤普森采样算法（采样计算每根拉杆的奖励概率）



#### 2023.10.20 (Dynamic Programming) 动态规划算法

基于动态规划的强化学习算法主要有两种：一是策略迭代（policy iteration），二是价值迭代（value iteration）。

其中，策略迭代由两部分组成：策略评估（policy evaluation）和策略提升（policy improvement）。

具体来说，策略迭代中的策略评估使用贝尔曼期望方程来得到一个策略的状态价值函数，这是一个动态规划的过程；而价值迭代直接使用贝尔曼最优方程来进行动态规划，得到最终的最优状态价值。

Project Path: RL-Learning/DynamicProgramming

条件：（model-based）
基于动态规划的这两种强化学习算法要求事先知道环境的状态转移函数和奖励函数，也就是需要知道整个马尔可夫决策过程。
在这样一个白盒环境中，不需要通过智能体和环境的大量交互来学习，可以直接用动态规划求解状态价值函数。